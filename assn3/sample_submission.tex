\documentclass[a4paper,11pt]{article}

\usepackage{mlsubmit}
\begin{document}

\initmlsubmision{3}                              					% assignment number
								{Nikhil Mittal}      						           		% your name
								{17111056}																		% your roll number

\begin{mlsolution}

To show :\\
\[
\theta^{MLE} \in \underset{\theta \; \in \; \Theta }{\arg \max} \; Q_{\theta^{MLE}} (\theta)\\
\]
Given, MLE model 
\[
\vtheta^{\text{MLE}} \in \Theta \text{ such that } \vtheta^{\text{MLE}} \in \underset{\vtheta \in \Theta}{\arg\max}\ \P{X\cond\vtheta}
\]

So,

\[
\underset{\theta \in \Theta}{max} \; \mathbb{P} [X|\theta] = \mathbb{P}[X|\theta^{MLE}]
\]

Taking log both sides :

\[
\underset{\theta \in \Theta}{max} \; \log\mathbb{P} [X|\theta] =\log \mathbb{P}[X|\theta^{MLE}] \;\;\;\;\;\;\;- (1)
\]

From Lecture 16,Slide 33
\[
\log\mathbb{P} [X|\theta] \geq Q_{\theta^t}(\theta) \;\forall \;\theta^t \in \Theta
\]

Since the above is true for any $\theta$ hence will be true for $\theta^{MLE}$ also.

\[
\log\mathbb{P} [X|\theta] \geq Q_{\theta^{MLE}}(\theta)
\]

Hence, 

\[
\underset{\theta \in \Theta}{\max} \;\log\mathbb{P} [X|\theta] \geq \underset{\theta \in \Theta}{\max} \;Q_{\theta^{MLE}}(\theta)
\]

Using (1), we get
\[
\log \mathbb{P}[X|\theta^{MLE}] \geq \underset{\theta \in \Theta}{\max} \;Q_{\theta^{MLE}}(\theta)   \;\;\;\;-(2)
\]

From Lecture16,Slide 44

We proved in class,

\[
\log \mathbb{P}[X|\theta^{0}] = \;Q_{\theta^{0}}(\theta^{0})
\]

Hence this will be true for $\theta^{MLE}$ also, since $\theta^{MLE} \in \Theta$

\[
\implies \log \mathbb{P}[X|\theta^{MLE}] = \;Q_{\theta^{MLE}}(\theta^{MLE})
\]\\

Using above result and (2), we get

\[
Q_{\theta^{MLE}}(\theta^{MLE}) \geq \underset{\theta \in \Theta}{\max} \;Q_{\theta^{MLE}}(\theta)
\]

Hence,
\[
\theta^{MLE} \in \underset{\theta \; \in \; \Theta }{\arg \max} \; Q_{\theta^{MLE}} (\theta)\\
\]

\end{mlsolution}

\begin{mlsolution}
An $n$-partition of a set $\cX$ is a collection of $n$ subsets $\bc{\cX_1,\ldots,\cX_n}$ such that each $\cX_i \subseteq \cX$ and
\begin{itemize}
    \item $\cX_i \cap \cX_j = \phi$ if $i \neq j$
    \item $\bigcup_{i=1}^n \cX_i = \cX$
\end{itemize}

\begin{enumerate}
\item
To show: for a linear piecewise function $f(x)$, $c \cdot f(x)$ is also piecewise linear.\

A piecewise linear function $f: \bR^d \rightarrow \bR$ with $n > 0$ ``pieces'' is indexed by an $n$-partition $\bc{\Omega_1,\ldots,\Omega_n}$ of $\bR^d$ and $n$ linear models $\vw^1,\ldots,\vw^n$ such that for any $\vx \in \bR^d$.
 
\[f(\vx) = \sum_{i=1}^n \ind{\vx \in \Omega_i}\cdot\ip{\vw^i}{\vx},\]

\begin{align}
 g(\vx) = c \cdot f(\vx)\\
 = c \cdot \sum_{i=1}^n \ind{\vx \in \Omega_i}\cdot\ip{\vw^i}{\vx}\\
 = \sum_{i=1}^n \ind{\vx \in \Omega_i}\cdot c \cdot \ip{\vw^i}{\vx}\\
 = \sum_{i=1}^n \ind{\vx \in \Omega_i}\cdot \ip{c \vw^i}{\vx} 
\end{align}

To define $g$ from the definition of $f$, we only require to update all $\vw^i $ to be $c\vw^i$, $c\vw^i$ is also a linear model. Hence this definition of $g$ does not change the partition and the piecewise nature of the function.\\
Hence $g(\vx) = c \cdot f(\vx) $ is a piecewise function defined on same partition set with scaled model vectors.

\item
To show, sum of 2 piecewise linear functions is also linear.

Let,  
\[f(\vx) = \sum_{i=1}^n \ind{\vx \in \Omega_i^f}\cdot\ip{\vw^i}{\vx},\]
defined on an $n$-partition of $\bD $ is a collection of $n$ subsets $\bc{\Omega_1^f,\ldots,\Omega_n^f}$ and set of model vectors $\bc {\vw^1, \ldots \vw^n }$

and

\[g(\vx) = \sum_{i=1}^m \ind{\vx \in \Omega_i^g}\cdot\ip{\vv^i}{\vx},\]
defined on an $n$-partition of $\bD $ is a collection of $n$ subsets $\bc{\Omega_1^g,\ldots,\Omega_m^g}$ and set of model vectors $\bc {\vv^1, \ldots \vv^m }$

to define the sum of above function, $(f+g)(x) = f(x)+g(x)$, we define new partition set as:
$ \Omega  = \bc {\Omega_{11}, \Omega_{12} \ldots \Omega_{1m} \ldots \Omega_{nm}} $

where,
$x \in \Omega_{ij} \text {\textit{ iff}}  x \in \Omega_i^f  \textbf{ and } x \in \Omega_j^g$

\begin{claim}
$\Omega$ is a partition of $\vD$
\end{claim}
\begin{proof}
\begin{enumerate}

\item To show : $\Omega_{ij} \cap \Omega_{pq} = \phi \text{\textbf{\textit{ iff }}}  i \neq p \text{\textbf{ OR }} j \neq m $

let, $\Omega_{ij} \cap \Omega_{pq} \neq \phi$ for some $i,j,p,q$
\begin{align}
i.e., \vx \in \Omega_{ij} \cap \Omega_{pq}\\
\implies \vx \in \Omega_{ij}\\
\implies \vx \in \Omega_{i}^f \text{ and } \vx \in \Omega_{j}^g\\
\text{also, } \vx \in \Omega_{pq}\\
\implies \vx \in \Omega_{p}^f \text{ and } \vx \in \Omega_{q}^g
\end{align}
now if either $i \neq p$ or $j \neq q$ we have at least one common element in intersection of 2 different partitions of either $f$ or $g$, contradicting that $\Omega^f$ and $\Omega^g$ are partitions of $\vD$.

\item $\bigcup_{i=1, j=1}^{i=n, j=m} \Omega_{ij} = \vD$
\iffalse
To see that $\Omega$ spans whole $\vD$, notice that the construction is similar to getting all cartesian products of $\Omega^f$ and $\Omega^g$,
\fi

\[ \forall \vx \in \vD, \exists i \vx \in \Omega^f_i \]
\[ \forall \vx \in \vD, \exists j \vx \in \Omega^g_j \]
\[ \implies \forall \vx \in \vD \exists i,j \vx \in \Omega_{ij} \]
\end{enumerate}
\end{proof}
New partition is a valid partition for the "sum of functions" definition below:
\[ 
    f(\vx) + g(\vx) = \sum_{i=1}^{m} \sum_{j=1}^{n} \ind{\vx \in \Omega_{ij}} ( \ip{\vw^i}{\vx} + \ip{\vv^j}{\vx}
\]

\[ 
    = \sum_{i=1}^{m} \sum_{j=1}^{n} \ind{\vx \in \Omega_{ij}} ( \ip{\vw^i + \vv^j}{\vx} )
\]

\item
To show: if $f(\vx)$ is peicewise linear then $g(\vx) = f_{ReLU}(f(\vx)) $ is piecewise linear as well.
\[ 
    f_{ReLU}(f(\vx)) = \max(f(\vx), 0)
\]

$\forall \Omega_i \in \Omega$ partition $\Omega_i$ into 
\[ \Omega_i^{0} = \bc{ \vx's | f(\vx) < 0} \]
and
\[ \Omega_i^{1} = \bc{ \vx's | f(\vx) \geq 0} \]

Note that some of new partitions might be empty, but that does not break the definition of partition given above.

And $\vw$ is updated as $ \vw^{i0}  = \textbf{0}$ and $\vw^{i1} = \vw^i$

We have new $n'$-partition of the domain, with new $n'$ model vectors.

With updated $\Omega$ and $\vw$ $f_{ReLU}$ is 

$f_{ReLU}(f(x)) = \sum_{i=1}^{n'} \ind{\vx \in \Omega_{i}} ( \ip{\vw^i}{\vx} )$ which is piecewise linear with new $\Omega$.

\item To show that neural nets with $f_{ReLU}$ activation constructs piecewise linear functions


\textbf{Proof idea:} From (1) and (2) we have shown that any linear combination of piecewise linear functions is also piecewise linear. Also in part $(3)$ we proved, $f_{ReLU}(f(\vx))$ is 

\textit{Proof by Induction on number of layers}

Base case:

let NN be a neural net with only one layer of activation. 


\begin{figure}[th]%
\centering
\includegraphics[width=0.8\columnwidth]{nn.png}%
\hfill
\caption{}%
\label{fig:proto}%
\end{figure}

Evaluation, $ \sum_{i=1}^{n_1}{\vw_i \cdot \vx_i }$ is a Piecewise Linear function.

and the activation layer is $f_{ReLU}(g(\vx))$.

From part $(3)$ we conclude output for above neural network is linearly piecewise.

$P(1)$ is \textit{true}.

Let's assume this claim is true for neural nets with $m$ layers. And let $z_i$ represent output of $i^{th}$ component of $m^{th}$ layer.

Now adding a new layer $m+1$ above $m^{th}$ layer can be seen as: 

Evaluation:
$g = \sum_{i}{w^m_i \cdot z_i}$

from $(1)$ and $(2)$, if $f(x)$ and $h(x)$ are piecewise linear, then so will be $g(x) = a \cdot f(x) + b \cdot h(x)$ for any scalars $ a, b$.

given the $m^{th} $ layer outputs from each node as a piecewise linear function \iffalse (\textit{by induction hypothesis})\fi , Evaluation in $(m+1)^{st}$ layer is also piecewise linear.

Next, (3) shows $f_{ReLU} (f(\vx))$ is piecewise linear for any piecewise linear $f(\vx)$,
$implies$ output of $(m+1)^{st}$ layer is piecewise linear as well.



\end{enumerate}
\end{mlsolution}


\begin{mlsolution}
\begin{mlalgorithm}[0.9\textwidth]{h!}{kernelized perceptron}\vskip-2ex
	\label{algo:tk-means}
	\begin{algorithmic}[1]
		\REQUIRE Data $\vx^1,\ldots,\vx^n$ in online way
		\STATE $\mathbf{\alpha} \leftarrow 0$\hfill //Initialize as 0 vector
		
		\STATE Receive data point $z^{t} = (x^{t}, y^{t})$\newline
// Compute Activation i.e. $\left \langle w, \phi(x^t) \right \rangle$, no need to compute feature map. By Perceptron Representer Theorem $w = \sum_{m} \alpha_{m} \phi(x^{m})$.
		So, $\left \langle w, \phi(x^t) \right \rangle = \left \langle \sum_{m} \alpha_{m} \phi(x^m), \phi(x^t) \right \rangle = \sum_{m} \alpha_{m} \left \langle  \phi(x^m), \phi(x^t) \right \rangle$ which is $\sum_{m} \alpha_{m} K( x^{m}, x^{t})$\newline
		\STATE Compute a $\leftarrow \sum_{m} \alpha_{m} K( x^{m}, x^{t})$ \\ 
		\\
		\IF{$y^{t}a \leq 0$}
		\STATE  $\alpha_{t} = \alpha_{t} + y^{t}$ // Make updates only when making a mistake\\
		
		\ENDIF
	\end{algorithmic}
\end{mlalgorithm}

By Perceptron Representer Theorem, 
\[
    w = \sum_{m} \alpha_{m} \phi(x^{m})
\]

So calculating $w$ not needed, its expression can be directly used to get value $<w, \phi(x)>$ at test time.

So for a test point $x$,
\[
\left \langle w, \phi(x) \right \rangle \;= \;\left \langle \sum_{m} \alpha_{m} \phi(x_{m}), \phi(x)\right \rangle\;
            = \;\sum_{m} \alpha_{m} \left \langle \phi(x_{m}), \phi(x) \right \rangle
\]

then,

\[
\left \langle w, \phi(x) \right \rangle \;= \;\sum_{m} \alpha_{m} K( x^{m}, x)
\]

\end{mlsolution}

\begin{mlsolution}

3.4.1

\[
K(z^{1}, z^{2}) = (\left \langle z^{1}, z^{2} \right \rangle + 1)^{2}
\]

where \[z^{1} = (x^{1}, y^{1}), z^{2}=(x^{2}, y^{2})\]

\[

(\left \langle z^{1}, z^{2} \right \rangle + 1)^{2} = 1 + 2 \left \langle  z^{1}, z^{2} \right \rangle + \left \langle  z^{1}, z^{2} \right \rangle^{2}\\
\]
\[
= 1  + 2(x^1x^2 + y^1y^2) + (x^1x^2 + y^1y^2)^2
\]
\[
= 1 + 2x^1x^2 + 2y^1y^2 + (x^1x^2)^2 + (y^1y^2)^2 + 2x^1x^2y^1y^2
\]
\[
= 1 + 2x^1x^2 + 2y^1y^2 + (x^1x^2)^2 + (y^1y^2)^2 + x^1x^2y^1y^2 + x^1x^2y^1y^2
\]
\[
= 1.1 + \sqrt{2}x^1.\sqrt{2}x^2 + \sqrt{2}y^1.\sqrt{2}y^2 + (x^1)^2.(x^2)^2 + (y^1)^2.(y^2)^2 + (x^1x^2).(y^1y^2) + (x^2x^1).(y^2y^1)
\]

\[
= (1, \sqrt{2}x^1, \sqrt{2}y^1,(x^1)^2, (y^1)^2, x^1y^1, y^1x^1)^T(1, \sqrt{2}x^2, \sqrt{2}y^2,(x^2)^2, (y^2)^2, x^2y^2, y^2x^2)
\]

\[
= \left \langle \phi(z^1), \phi(z^2) \right \rangle
\]

where 
\[

\phi(z^1) = (1, \sqrt{2}x^1, \sqrt{2}y^1,(x^1)^2, (y^1)^2, x^1y^1, y^1x^1)

\]

$\mathcal{H}_{K}$ = R^{7}

So, D is 7.
\newpage
3.4.2

Let $w = \left (  w_1, w_2, w_3, w_4, w_5, w_6, w_7\right )$\\

and

\[
\phi(z^1) = (1, \sqrt{2}x^1, \sqrt{2}y^1,(x^1)^2, (y^1)^2, x^1y^1, y^1x^1)
\]

$\left \langle w, \phi(z) \right \rangle = w_1 + \sqrt{2}xw_2 + \sqrt{2}yw_3 + x^2w_4 + xyw_5 + yxw_6 + y^2w_7$
\\

Let
\[
A =\begin{bmatrix}
a & b'\\ 
c' & d
\end{bmatrix}
\\
and\;
b = \begin{bmatrix}
e\\
f
\end{bmatrix}
\]

\[
z = \begin{bmatrix}
x\\
y
\end{bmatrix}
\]

So,  
\[
\left \langle z, Az \right \rangle = \begin{bmatrix}
x\\
y
\end{bmatrix}^T\begin{bmatrix}
a & b'\\ 
c' & d
\end{bmatrix}*\begin{bmatrix}
x\\
y
\end{bmatrix}
\]

\[
\left \langle b, z \right \rangle = ex + fy
\]

\[
f_(_A_,_b_,_c_)(z) = ax^2 + b'xy + c'yx + dy^2 + ex + fy + c
\]

Comparing
\[
\left \langle w, \phi(z) \right \rangle \;and\; f_(_A_,_b_,_c_)(z)
\]

we get, (on comparing coefficients of x, y, x^2, y^2, xy and constant term)

So, 

\[
    w_1 = c 
\]
\[
    \sqrt{2}x w_2  = ex
\]
\[
    \sqrt{2}y w_3  = fy
\]
\[
x^2w_4 = ax^2
\]
\[
xyw_5 = b'xy
\]
\[
yxw_6 = c'yx
\]
\[
y^2w_7 = dy^2
\]

Hence,
\[
w_1 = c, w_2 = e/\sqrt{2}, w_3 = f/\sqrt{2}, w_4 = a, w_5 = b', w_6 = c'
, w_7 = d\]\\

$w = \left (  c, \frac{e}{\sqrt{2}}, \frac{f}{\sqrt{2}}, a, b', c', d\right )$\\

\newpage

3.4.3

Given, $w = \left (  w_1, w_2, w_3, w_4, w_5, w_6, w_7\right )$\\

Let
\[
A =\begin{bmatrix}
a & b'\\ 
c' & d
\end{bmatrix}
\\
and\;
b = \begin{bmatrix}
e\\
f
\end{bmatrix}
\]

\[
z = \begin{bmatrix}
x\\
y
\end{bmatrix}
\]

So,  
\[
\left \langle z, Az \right \rangle = \begin{bmatrix}
x\\
y
\end{bmatrix}^T\begin{bmatrix}
a & b'\\ 
c' & d
\end{bmatrix}*\begin{bmatrix}
x\\
y
\end{bmatrix}
\]

\[
\left \langle b, z \right \rangle = ex + fy
\]

\[
f_(_A_,_b_,_c_)(z) = ax^2 + b'xy + c'yx + dy^2 + ex + fy + c
\]

Comparing
\[
f_(_A_,_b_,_c_)(z) \;and\; \left \langle w, \phi(z) \right \rangle
\]


we get, (on comparing coefficients of x, y, x^2, y^2, xy \text{ and constant term})

\[
    c  = w_1
\]
\[
    ex = \sqrt{2}x w_2
\]
\[
    fy = \sqrt{2}y w_3 
\]
\[
    ax^2 = x^2w_4
\]
\[
    b'xy = xyw_5 
\]
\[
    c'yx = yxw_6
\]
\[
    dy^2 = y^2w_7
\]

Hence,
\[
c = w_1, e = \sqrt{2}w_2, f = \sqrt{2}w_3, a = w_4, b' = w_5, c' = w_6, d = w_7
\]\\

Therefore,

\[
A =\begin{bmatrix}
w_4 & w_5\\ 
w_6 & w_7
\end{bmatrix}
\\
and\;
b = \begin{bmatrix}
\sqrt{2}w_2\\
\sqrt{2}w_3
\end{bmatrix}
\]

and
\[
c = w_1
\]

\end{mlsolution}

\begin{mlsolution}

Given
\[
\P{\vz} = \cN(\vzero,I_k) \in \bR^k,
\]
whereupon an affine transformation is applied to them and noise is added to produce the observed data point, i.e. for $W \in \bR^{d\times k}, \vmu \in \bR^d, \sigma \geq 0$
\[
\P{\vx\cond\vz} = \cN(\vx\cond W\vz+\mu, \sigma^2\cdot I_d) \in \bR^d.
\]
Now using conjugacy properties of the Gaussian ([\textbf{BIS}] Chapter 12), we can show that
\[
\P{\vx} = \int_{\vz}\P{\vx\cond\vz}\P{\vz}\ d\vz = \cN(\vx\cond\vmu,C),
\]
where $C = WW^\top + \sigma^2\cdot I_d$. For a dataset $X = \bs{\vx^1,\ldots,\vx^n}$.\\

Here, To find the mean and covariance of data X :

\[
\mathbb{E}[\textbf{x}] = \mathbb{E}[Wz + \mu + \epsilon]
\]

By, linearity of expectaion,

\[
 =  \mathbb{E}[Wz] + \mathbb{E}[\mu] + \mathbb{E}[\epsilon] 
\]

Given,
\[
\mathbb{E}[z] = 0,\; \mathbb{E}[\epsilon]\; =\; 0 \;and \;\mathbb{E}[\mu]\; = \;\mu 
\]

So, 
\[
\mathbb{E}[\textbf{x}] =  \mathbb{E}[Wz] + \mathbb{E}[\mu] + \mathbb{E}[\epsilon] = \mu
\]

Now, Covariance 

\[
Cov[x] = \mathbb{E}[(Wz + \epsilon)(Wz + \epsilon)^T]
\]
\[
=  \mathbb{E}[(Wz + \epsilon)(z^TW^T + \epsilon^T)]
\]
\[
=  \mathbb{E}[(Wzz^TW^T + \epsilon\epsilon^T)]
\]
\[
=  \mathbb{E}[(WW^T + \epsilon\epsilon^T)]
\]
\[
=  \mathbb{E}[WW^T] + \mathbb{E}[\epsilon\epsilon^T]
\]
\[
=  WW^T + \sigma^2I_d
\]

Let,
 \[
    C = WW^T + \sigma^2I_d
 \]
Likelihood Expression : 

\[
\mathbb{P}(x^i | \mu, W, \sigma^2) = \frac{1}{\sqrt{(2\pi)^D |C|}} \exp\left ( -\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2} \right )
\]

Taking log 

\[
\log\mathbb{P}(x^i | \mu, W, \sigma^2) = -\frac{D}{2}\log2\pi - \frac{\log|C|}{2}   -\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2}
\]

\[
\sum_i \log\mathbb{P}(x^i | \mu, W, \sigma^2) = 
\sum_i \left (
-\frac{D}{2}\log2\pi - \frac{\log|C|}{2}   -\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2} \right )
\]

\[
\sum_i \log\mathbb{P}(x^i | \mu, W, \sigma^2) = 
-\frac{ND}{2}\log2\pi - \frac{N\log|C|}{2} - \sum_i\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2}
\]

\[
\mathbb{P}(X | \mu, W, \sigma^2) = \prod_i\mathbb{P}(x^i | \mu, W, \sigma^2) 
\]

\[
\log \mathbb{P}(X | \mu, W, \sigma^2) = \log \prod_i\mathbb{P}(x^i | \mu, W, \sigma^2) 
\]

\[
\log \mathbb{P}(X | \mu, W, \sigma^2) = \sum_i \log \mathbb{P}(x^i | \mu, W, \sigma^2) 
\]

\textbf{Complete Likelihood Expression : }

\[
\log \mathbb{P}(X | \mu, W, \sigma^2) =-\frac{ND}{2}\log2\pi - \frac{N\log|C|}{2} - \sum_i\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2}
\]

\[
\log \mathbb{P}(X | \mu, W, \sigma^2) =-\frac{N}{2} \left ( D\log2\pi + \log|C| + \frac{1}{N}\sum_i\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2} \right )
\]

\textbf{Derivation for } \textbf{\mu}^{MLE}:\\

Taking derivative w.r.t \mu

\[
\frac{d}{d\mu} \log \mathbb{P}(X | \mu, W, \sigma^2) = \frac{d}{d\mu} \left ( -\frac{N}{2} \left ( D\log2\pi + \log|C| + \frac{1}{N}\sum_i\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2} \right ) \right )
\]

\[
\frac{d}{d\mu} \log \mathbb{P}(X | \mu, W, \sigma^2) = \frac{-1}{2} \frac{d}{d\mu} \left ( \sum_i (x_i - \mu)^TC^{-1}(x_i - \mu) \right )
\]

Setting this to Zero,
\[
\frac{d}{d\mu} \left ( \sum_i (x_i - \mu)^TC^{-1}(x_i - \mu) \right ) = 0
\]

\[
\left ( \sum_i (-2 C^{-1})(x_i - \mu) \right ) = 0
\]

Hence,

\[
\sum_i (x_i - \mu) = 0
\]

\[
\sum_i (x_i) - \sum_i (\mu) = 0
\]

\[
\sum_i (x_i) - n\mu = 0
\]

So,
\[
\mu = \frac{1}{n}\sum_i x_i
\]

The Expression is :

\[
\mu^{MLE} = \frac{1}{n}\sum_i x_i
\]

\end{mlsolution}

\end{document}