\documentclass[a4paper,11pt]{article}

\usepackage{mlsubmit}
\begin{document}

\initmlsubmision{3}                              					% assignment number
								{Nikhil Mittal}      						           		% your name
								{17111056}																		% your roll number

\begin{mlsolution}
\end{mlsolution}

\begin{mlsolution}
\end{mlsolution}

\begin{mlsolution}
\begin{mlalgorithm}[0.9\textwidth]{h!}{kernelized perceptron}\vskip-2ex
	\label{algo:tk-means}
	\begin{algorithmic}[1]
		\REQUIRE Data $\vx^1,\ldots,\vx^n$ in online way
		\STATE $\mathbf{\alpha} \leftarrow 0$\hfill //Initialize as 0 vector
		
		\STATE Receive data point $z^{t} = (x^{t}, y^{t})$\newline
// Compute Activation i.e. $\left \langle w, \phi(x^t) \right \rangle$, no need to compute feature map. By Perceptron Representer Theorem $w = \sum_{m} \alpha_{m} \phi(x^{m})$.
		So, $\left \langle w, \phi(x^t) \right \rangle = \left \langle \sum_{m} \alpha_{m} \phi(x^m), \phi(x^t) \right \rangle = \sum_{m} \alpha_{m} \left \langle  \phi(x^m), \phi(x^t) \right \rangle$ which is $\sum_{m} \alpha_{m} K( x^{m}, x^{t})$\newline
		\STATE Compute a $\leftarrow \sum_{m} \alpha_{m} K( x^{m}, x^{t})$ \\ 
		\\
		\IF{$y^{t}a \leq 0$}
		\STATE  $\alpha_{t} = \alpha_{t} + y^{t}$ // Make updates only when making a mistake\\
		
		\ENDIF
	\end{algorithmic}
\end{mlalgorithm}

By Perceptron Representer Theorem, 
\[
    w = \sum_{m} \alpha_{m} \phi(x^{m})
\]

So calculating $w$ not needed, its expression can be directly used to get value $<w, \phi(x)>$ at test time.

So for a test point $x$,
\[
\left \langle w, \phi(x) \right \rangle \;= \;\left \langle \sum_{m} \alpha_{m} \phi(x_{m}), \phi(x)\right \rangle\;
            = \;\sum_{m} \alpha_{m} \left \langle \phi(x_{m}), \phi(x) \right \rangle
\]

then,

\[
\left \langle w, \phi(x) \right \rangle \;= \;\sum_{m} \alpha_{m} K( x^{m}, x)
\]

\end{mlsolution}

\begin{mlsolution}

3.4.1

\[
K(z^{1}, z^{2}) = (\left \langle z^{1}, z^{2} \right \rangle + 1)^{2}
\]

where \[z^{1} = (x^{1}, y^{1}), z^{2}=(x^{2}, y^{2})\]

\[

(\left \langle z^{1}, z^{2} \right \rangle + 1)^{2} = 1 + 2 \left \langle  z^{1}, z^{2} \right \rangle + \left \langle  z^{1}, z^{2} \right \rangle^{2}\\
\]
\[
= 1  + 2(x^1x^2 + y^1y^2) + (x^1x^2 + y^1y^2)^2
\]
\[
= 1 + 2x^1x^2 + 2y^1y^2 + (x^1x^2)^2 + (y^1y^2)^2 + 2x^1x^2y^1y^2
\]
\[
= 1 + 2x^1x^2 + 2y^1y^2 + (x^1x^2)^2 + (y^1y^2)^2 + x^1x^2y^1y^2 + x^1x^2y^1y^2
\]
\[
= 1.1 + \sqrt{2}x^1.\sqrt{2}x^2 + \sqrt{2}y^1.\sqrt{2}y^2 + (x^1)^2.(x^2)^2 + (y^1)^2.(y^2)^2 + (x^1x^2).(y^1y^2) + (x^2x^1).(y^2y^1)
\]

\[
= (1, \sqrt{2}x^1, \sqrt{2}y^1,(x^1)^2, (y^1)^2, x^1y^1, y^1x^1)^T(1, \sqrt{2}x^2, \sqrt{2}y^2,(x^2)^2, (y^2)^2, x^2y^2, y^2x^2)
\]

\[
= \left \langle \phi(z^1), \phi(z^2) \right \rangle
\]

where 
\[

\phi(z^1) = (1, \sqrt{2}x^1, \sqrt{2}y^1,(x^1)^2, (y^1)^2, x^1y^1, y^1x^1)

\]

$\mathcal{H}_{K}$ = R^{7}

So, D is 7.
\newpage
3.4.2

Let $w = \left (  w_1, w_2, w_3, w_4, w_5, w_6, w_7\right )$\\

and

\[
\phi(z^1) = (1, \sqrt{2}x^1, \sqrt{2}y^1,(x^1)^2, (y^1)^2, x^1y^1, y^1x^1)
\]

$\left \langle w, \phi(z) \right \rangle = w_1 + \sqrt{2}xw_2 + \sqrt{2}yw_3 + x^2w_4 + xyw_5 + yxw_6 + y^2w_7$
\\

Let
\[
A =\begin{bmatrix}
a & b'\\ 
c' & d
\end{bmatrix}
\\
and\;
b = \begin{bmatrix}
e\\
f
\end{bmatrix}
\]

\[
z = \begin{bmatrix}
x\\
y
\end{bmatrix}
\]

So,  
\[
\left \langle z, Az \right \rangle = \begin{bmatrix}
x\\
y
\end{bmatrix}^T\begin{bmatrix}
a & b'\\ 
c' & d
\end{bmatrix}*\begin{bmatrix}
x\\
y
\end{bmatrix}
\]

\[
\left \langle b, z \right \rangle = ex + fy
\]

\[
f_(_A_,_b_,_c_)(z) = ax^2 + b'xy + c'yx + dy^2 + ex + fy + c
\]

Comparing
\[
\left \langle w, \phi(z) \right \rangle \;and\; f_(_A_,_b_,_c_)(z)
\]

we get, (on comparing coefficients of x, y, x^2, y^2, xy and constant term)

So, 

\[
    w_1 = c 
\]
\[
    \sqrt{2}x w_2  = ex
\]
\[
    \sqrt{2}y w_3  = fy
\]
\[
x^2w_4 = ax^2
\]
\[
xyw_5 = b'xy
\]
\[
yxw_6 = c'yx
\]
\[
y^2w_7 = dy^2
\]

Hence,
\[
w_1 = c, w_2 = e/\sqrt{2}, w_3 = f/\sqrt{2}, w_4 = a, w_5 = b', w_6 = c'
, w_7 = d\]\\

$w = \left (  c, \frac{e}{\sqrt{2}}, \frac{f}{\sqrt{2}}, a, b', c', d\right )$\\

\newpage

3.4.3

Given, $w = \left (  w_1, w_2, w_3, w_4, w_5, w_6, w_7\right )$\\

Let
\[
A =\begin{bmatrix}
a & b'\\ 
c' & d
\end{bmatrix}
\\
and\;
b = \begin{bmatrix}
e\\
f
\end{bmatrix}
\]

\[
z = \begin{bmatrix}
x\\
y
\end{bmatrix}
\]

So,  
\[
\left \langle z, Az \right \rangle = \begin{bmatrix}
x\\
y
\end{bmatrix}^T\begin{bmatrix}
a & b'\\ 
c' & d
\end{bmatrix}*\begin{bmatrix}
x\\
y
\end{bmatrix}
\]

\[
\left \langle b, z \right \rangle = ex + fy
\]

\[
f_(_A_,_b_,_c_)(z) = ax^2 + b'xy + c'yx + dy^2 + ex + fy + c
\]

Comparing
\[
f_(_A_,_b_,_c_)(z) \;and\; \left \langle w, \phi(z) \right \rangle
\]


we get, (on comparing coefficients of x, y, x^2, y^2, xy \text{ and constant term})

\[
    c  = w_1
\]
\[
    ex = \sqrt{2}x w_2
\]
\[
    fy = \sqrt{2}y w_3 
\]
\[
    ax^2 = x^2w_4
\]
\[
    b'xy = xyw_5 
\]
\[
    c'yx = yxw_6
\]
\[
    dy^2 = y^2w_7
\]

Hence,
\[
c = w_1, e = \sqrt{2}w_2, f = \sqrt{2}w_3, a = w_4, b' = w_5, c' = w_6, d = w_7
\]\\

Therefore,

\[
A =\begin{bmatrix}
w_4 & w_5\\ 
w_6 & w_7
\end{bmatrix}
\\
and\;
b = \begin{bmatrix}
\sqrt{2}w_2\\
\sqrt{2}w_3
\end{bmatrix}
\]

and
\[
c = w_1
\]

\end{mlsolution}

\begin{mlsolution}

Given
\[
\P{\vz} = \cN(\vzero,I_k) \in \bR^k,
\]
whereupon an affine transformation is applied to them and noise is added to produce the observed data point, i.e. for $W \in \bR^{d\times k}, \vmu \in \bR^d, \sigma \geq 0$
\[
\P{\vx\cond\vz} = \cN(\vx\cond W\vz+\mu, \sigma^2\cdot I_d) \in \bR^d.
\]
Now using conjugacy properties of the Gaussian ([\textbf{BIS}] Chapter 12), we can show that
\[
\P{\vx} = \int_{\vz}\P{\vx\cond\vz}\P{\vz}\ d\vz = \cN(\vx\cond\vmu,C),
\]
where $C = WW^\top + \sigma^2\cdot I_d$. For a dataset $X = \bs{\vx^1,\ldots,\vx^n}$.\\

Here, To find the mean and covariance of data X :

\[
\mathbb{E}[\textbf{x}] = \mathbb{E}[Wz + \mu + \epsilon]
\]

By, linearity of expectaion,

\[
 =  \mathbb{E}[Wz] + \mathbb{E}[\mu] + \mathbb{E}[\epsilon] 
\]

Given,
\[
\mathbb{E}[z] = 0,\; \mathbb{E}[\epsilon]\; =\; 0 \;and \;\mathbb{E}[\mu]\; = \;\mu 
\]

So, 
\[
\mathbb{E}[\textbf{x}] =  \mathbb{E}[Wz] + \mathbb{E}[\mu] + \mathbb{E}[\epsilon] = \mu
\]

Now, Covariance 

\[
Cov[x] = \mathbb{E}[(Wz + \epsilon)(Wz + \epsilon)^T]
\]
\[
=  \mathbb{E}[(Wz + \epsilon)(z^TW^T + \epsilon^T)]
\]
\[
=  \mathbb{E}[(Wzz^TW^T + \epsilon\epsilon^T)]
\]
\[
=  \mathbb{E}[(WW^T + \epsilon\epsilon^T)]
\]
\[
=  \mathbb{E}[WW^T] + \mathbb{E}[\epsilon\epsilon^T]
\]
\[
=  WW^T + \sigma^2I_d
\]

Let,
 \[
    C = WW^T + \sigma^2I_d
 \]
Likelihood Expression : 

\[
\mathbb{P}(x^i | \mu, W, \sigma^2) = \frac{1}{\sqrt{(2\pi)^D |C|}} \exp\left ( -\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2} \right )
\]

Taking log 

\[
\log\mathbb{P}(x^i | \mu, W, \sigma^2) = -\frac{D}{2}\log2\pi - \frac{\log|C|}{2}   -\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2}
\]

\[
\sum_i \log\mathbb{P}(x^i | \mu, W, \sigma^2) = 
\sum_i \left (
-\frac{D}{2}\log2\pi - \frac{\log|C|}{2}   -\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2} \right )
\]

\[
\sum_i \log\mathbb{P}(x^i | \mu, W, \sigma^2) = 
-\frac{ND}{2}\log2\pi - \frac{N\log|C|}{2} - \sum_i\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2}
\]

\[
\mathbb{P}(X | \mu, W, \sigma^2) = \prod_i\mathbb{P}(x^i | \mu, W, \sigma^2) 
\]

\[
\log \mathbb{P}(X | \mu, W, \sigma^2) = \log \prod_i\mathbb{P}(x^i | \mu, W, \sigma^2) 
\]

\[
\log \mathbb{P}(X | \mu, W, \sigma^2) = \sum_i \log \mathbb{P}(x^i | \mu, W, \sigma^2) 
\]

\textbf{Complete Likelihood Expression : }

\[
\log \mathbb{P}(X | \mu, W, \sigma^2) =-\frac{ND}{2}\log2\pi - \frac{N\log|C|}{2} - \sum_i\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2}
\]

\[
\log \mathbb{P}(X | \mu, W, \sigma^2) =-\frac{N}{2} \left ( D\log2\pi + \log|C| + \frac{1}{N}\sum_i\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2} \right )
\]

\textbf{Derivation for } \textbf{\mu}^{MLE}:\\

Taking derivative w.r.t \mu

\[
\frac{d}{d\mu} \log \mathbb{P}(X | \mu, W, \sigma^2) = \frac{d}{d\mu} \left ( -\frac{N}{2} \left ( D\log2\pi + \log|C| + \frac{1}{N}\sum_i\frac{(x_i - \mu)^TC^{-1}(x_i - \mu)}{2} \right ) \right )
\]

\[
\frac{d}{d\mu} \log \mathbb{P}(X | \mu, W, \sigma^2) = \frac{-1}{2} \frac{d}{d\mu} \left ( \sum_i (x_i - \mu)^TC^{-1}(x_i - \mu) \right )
\]

Setting this to Zero,
\[
\frac{d}{d\mu} \left ( \sum_i (x_i - \mu)^TC^{-1}(x_i - \mu) \right ) = 0
\]

\[
\left ( \sum_i (-2 C^{-1})(x_i - \mu) \right ) = 0
\]

Hence,

\[
\sum_i (x_i - \mu) = 0
\]

\[
\sum_i (x_i) - \sum_i (\mu) = 0
\]

\[
\sum_i (x_i) - n\mu = 0
\]

So,
\[
\mu = \frac{1}{n}\sum_i x_i
\]

The Expression is :

\[
\mu^{MLE} = \frac{1}{n}\sum_i x_i
\]

\end{mlsolution}
\end{document}\\
