\documentclass[a4paper,11pt]{article}

\usepackage{mlsubmit}

\begin{document}

\initmlsubmision{2}                              					% assignment number
								{Nikhil Mittal}      						           		% your name
								{17111056}																		% your roll number

\begin{mlsolution}

1. Answer : NO. It is useful only when the Name exists in the data, but if the name is not present then we can't classify based on name. So the first attribute (name) is not useful in learning.
If we build a decision tree using the attribute name then the branching factor will be such high that it will shatter any dataset and overfit.
\\

2. Answer : Yes.\\

3. Decision Tree obtained after ID3 decision tree learning algorithm is shown in Figure 1. Steps for producing this are :
\\\\Attributes are : Size of research group, Like the research area, Average workload, No. of meetings  per week. Suppose S is this full data given. Then, \\\\Entropy(S) = 0.9182\\
Gain(S, Size of research group) = 0.0679\\
Gain(S, Like the research area) = 0.03170\\
Gain(S, Average workload) = 0.06131\\
Gain(S, No. of meetings  per week) = 0.2516\\\\So Root node is No. of meetings per week as it has the highest Gain.\\\\Since "No. of meetings per week" has three possible values, the root node has three branches (2-3, 0-1, $>$3). All rows with 2-3 meetings per week give result "NO", and similarly on taking branch "$>$3" we get "NO" as answer. \\\\We only decide on the remaining three attributes: Size of research group, Like the research area, Average workload. S is now the remaining set under branch "0-1" (No. of meetings per week).\\
Entropy(S) = 1.0\\
Gain(S, Size of research group) = 0.11452\\
Gain(S, Like the research area) = 0.0348\\
Gain(S, Average workload) = 0.439\\\\Selecting Average workload as the decision node. Splitting into branches upon its possible values. "Average" workload branch leads to always "Yes" as answer.\\Still need to decide for "Heavy" and "Light" average workload branch.\\\\For "Heavy" (Average workload) branch :\\Entropy(S) = 0.7219\\
Gain(S, Size of research group) = 0.1709\\
Gain(S, Like the research area) = 0.0729\\\\Selecting Size of research group as the decision node, highest gain.
Its branches are Medium and Large. Large always yields "NO" and Medium gives "YES" for 1 data point and "NO" for 2 data points, hence choosing "NO" by majority.\\\\For "Light" (Average workload) branch :\\
Entropy(S) = 1.0\\
Gain(S, Size of research group) = 2.0\\
Gain(S, Like the research area) = 1.0\\\\Selecting Size of research group as the decision node, highest gain. We have only Medium size as branch and it's values are "YES" and "NO" one for each data point. So arbitrarily breaking tie by choosing "YES".\\\\Since we have to stop splitting nodes that are at depth 2 or more. Hence this "Light" Average workload branch always results in "YES" as answer, hence assigning directly to "YES" without any further split. Similarly the "Heavy" Average workload branch always results in "NO" as answer irrespective of the "Size of Research group" being Medium or Large, hence directly assigning the leaf node answer "NO" without any further split.\\\\These steps give us the decision tree as shown in figure 1.

\begin{figure}[th]%
\centering
\includegraphics[width=0.5\columnwidth]{tree.png}%

\caption{Decision Tree}%
\label{fig:Dtree}%
\end{figure}



\end{mlsolution}

\begin{mlsolution}
second
\end{mlsolution}

\begin{mlsolution}

\[
f(w) = \arg \underset{w, {\xi_{i} }}{\min} \left \| w \right \|_{2}^{2} + \sum_{i = 1}^{n} \xi^{2}
\]\[
\\
s.t. y^{i} \left \langle w, x^{i} \right \rangle \geq 1 - \xi_{i}, \text{for} \; \text{all} \; i \in [n]
\]\[\\
\xi_{i} \geq 0\; \text{for} \; \text{all} \; i \in [n]
\]

2.3.2: Answer :\\

Lagrangian for (P1) :

\[
\mathcal{L}(w, \xi, \alpha, \beta) = \arg \underset{w, {\xi_{i} }}{ \text{min} } \; \underset{\alpha\geq 0 ,\; \beta\geq0}{\text{max}} \; \left \| w \right \|_{2}^{2} + \sum_{i = 1}^{n} \xi^{2}
 - \sum_{i=1}^{n} \alpha_{i} \left ( y^{i} \left \langle w, x^{i} \right \rangle - 1 + \xi_{i} \right ) - \sum_{i=1}^{n}\beta_{i} \xi_{i}
\]

2.3.3: Answer :\\

Derivation for Dual problem for (P1)\\

We can solve this problem by taking gradients.Since our goal is to remove the dependence on w, the first step is to take a gradient with respect to w, set it equal to zero, and solve for w in terms of the other variables.\\
\[
\frac{\partial  \mathcal{L}}{\partial w} = 2w - \sum_{i=1}^{n} \alpha_{i}y^{i}x^{i} 
\]\\\[
\frac{\partial  \mathcal{L}}{\partial w}  = 0
\]\\\[
w = \frac{1}{2}\sum_{i=1}^{n} \alpha_{i}y^{i}x^{i} 
\]

Also to remove the dependence on $\xi$, take a gradient with respect to $\xi$, set it equal to zero, and solve for $\xi$ in terms of the other variables.

\[
\frac{\partial  \mathcal{L}}{\partial \xi_{i} } = 2\;\xi_{i} - \alpha_{i} - \beta_{i}
\]\\\[
\frac{\partial  \mathcal{L}}{\partial \xi_{i} } = 0
\]\\\[
\xi_{i} = \frac{\alpha_{i} + \beta_{i}}{2}
\]

Substitute w and $\xi$ back in $\mathcal{L}(w, \xi, \alpha, \beta)$ and eliminate them.

\[
  \mathcal{L}\left ( \alpha,\; \beta  \right ) = \underset{\alpha\geq 0, \beta \geq 0 }{\text{max}} \left ( \frac{1}{2}\sum_{i=1}^{n} \alpha_{i}y^{i}x^{i}  \right )\left ( \frac{1}{2}\sum_{j=1}^{n} \alpha_{j}y^{j}x^{j} \right ) + \sum_{i=1}^{n} \left ( \frac{\alpha_{i} + \beta_{i}}{2} \right )^{2}  \]\[ - \sum_{i=1}^{n} \alpha_{i}\left ( y^{i}\left ( \frac{1}{2}\sum_{j=1}^{n} \alpha_{j}y^{j}x^{j}  \right )x^{i} - 1  +  \frac{\alpha_{i} + \beta_{i}}{2} \right ) - \sum_{i=1}^{n} \beta_{i} \left ( \frac{\alpha_{i} + \beta_{i}}{2}  \right )
\]\\

\[
  \mathcal{L}\left ( \alpha,\; \beta  \right ) = \underset{\alpha\geq 0, \beta \geq 0 }{\text{max}} \; \frac{1}{4}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j}y^{i}y^{j}\left \langle x^{i}, x^{j} \right \rangle+ \sum_{i=1}^{n} \left ( \frac{\alpha_{i} + \beta_{i}}{2} \right )^{2} -\; \frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j}y^{i}y^{j}\left \langle x^{i}, x^{j} \right \rangle \]\[ + \sum_{i=1}^{n}\alpha_{i} - \sum_{i=1}^{n} \alpha_{i} \left ( \frac{\alpha_{i} + \beta_{i}}{2}  \right ) - \sum_{i=1}^{n} \beta_{i} \left ( \frac{\alpha_{i} + \beta_{i}}{2}  \right )
\]\\

\[
  \mathcal{L}\left ( \alpha,\; \beta  \right ) = \underset{\alpha\geq 0, \beta \geq 0 }{\text{max}} \; -\frac{1}{4}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j}y^{i}y^{j}\left \langle x^{i}, x^{j} \right \rangle+ \sum_{i=1}^{n}\alpha_{i} - \frac{1}{4}\sum_{i=1}^{n} \left ( \frac{\alpha_{i} + \beta_{i}}{2} \right )^{2}
\]\\

$\mathcal{L}(\alpha, \beta)$ is the expression for dual of $\mathcal{L}(w, \xi, \alpha, \beta)$.\\

2.3.1: Answer : \\

Using the expression obtained for the Lagrangian dual problem for (P1).

We have 
\[
  \mathcal{L}\left ( \alpha,\; \beta  \right ) = \underset{\alpha\geq 0, \beta \geq 0 }{\text{max}} \; -\frac{1}{4}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_{i}\alpha_{j}y^{i}y^{j}\left \langle x^{i}, x^{j} \right \rangle+ \sum_{i=1}^{n}\alpha_{i} - \frac{1}{4}\sum_{i=1}^{n} \left ( \frac{\alpha_{i} + \beta_{i}}{2} \right )^{2} \;\;\;\;\;\;\; - (1)
\]\\

We have to maximize for $\alpha$ and $\beta$ to get optimal value for dual problem.

So let's maximize for $\beta$ first.

\[
\frac{\partial \mathcal{L}\left ( \alpha,\; \beta  \right ) }{\partial w} = 0 + 0 -\frac{1}{4}\left ( 2(\alpha_{i} + \beta_{i})(1) \right ) \] \\

\text{set}\; \[\frac{\partial \mathcal{L}\left ( \alpha,\; \beta  \right ) }{\partial w} = 0
\]
we get,
\[
\alpha_{i} + \beta_{i} = 0
\]

This does allow us to substitute $\beta_{i} = - \alpha_{i}$ back in the equation (1). But what we can also notice
\end{mlsolution}

\begin{mlsolution}
 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis feugiat vehicula dolor, sed ultricies leo. Phasellus euismod dictum felis in euismod. Proin pretium vel neque in placerat. Proin imperdiet egestas vulputate. Etiam faucibus accumsan ante non viverra. Duis ultrices ac odio vel sodales. In maximus gravida dolor, ut commodo lacus. Pellentesque ante massa, venenatis id aliquam et, posuere sed dui. Duis dignissim justo sit amet augue posuere fringilla. Suspendisse at nisi gravida, mattis justo sit amet, elementum elit. Praesent et massa ornare, consequat dui eget, ornare risus. Duis est nibh, sollicitudin nec mattis non, mattis in leo. Donec finibus justo sed massa sagittis, non fermentum nibh dictum. Pellentesque et congue purus. Donec porta pretium porttitor.

Morbi euismod risus eu tortor ornare malesuada. Nunc sed sollicitudin neque, efficitur rhoncus tellus. Cras malesuada augue arcu. Sed sem odio, tincidunt quis laoreet ac, facilisis ut nibh. Quisque gravida dolor at egestas aliquam. Aenean mollis massa sit amet enim mattis, vel fermentum tortor facilisis. Donec pellentesque est velit, vitae posuere lorem tristique ut. 
\end{mlsolution}
					
\end{document}